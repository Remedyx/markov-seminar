\documentclass[11pt,a4paper, final]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage[UKenglish]{isodate}

\newtheorem{lem}{Lemma}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{cor}{Corollary}[section]
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{exmp}{Example}[section]
\newtheorem{rem}{Remark}[section]

\renewcommand\thesection{\arabic{section}}



\begin{document}
\section{Conditional Expection}
We start with some preliminaries, things known or less known.
\\\\
Let $( \Omega, \mathcal{F}, \mathbb{P})$ be a probability space
\begin{defn} We define the Lebesgue spaces \\ $L^p(\Omega, \mathcal{F}, \mathbb{P})=L^p= \lbrace X: \Omega \to \mathbb{R} \mid X \text{ is $\mathcal{F}$  measurable and } \mathbb{E}(|X|^p) < \infty \rbrace$ where $1 \leq p < \infty$. \\
$L^\infty = \lbrace X : \Omega \to \mathbb{R} \mid X \text{ is $\mathcal{F}$ measurable and } \exists C>0 : \mathbb{P}(|X| \leq C) = 1  \rbrace  $ is the space of almost surely bounded random variables. 
\end{defn}
\begin{thm}[Hölder's inequality] Let $X \in L^P, Y \in L^q$ for conjugate $p,q \geq 1$ (i.e. $1/p + 1/q = 1$) then we have $XY \in L^1$ moreover Hölder's inequality holds true
\begin{align*}
\mathbb{E}(|XY|) \leq \mathbb{E}(|X|^p)^{\frac{1}{p}} \mathbb{E}(|X|^q)^{\frac{1}{q}}
\end{align*}
\end{thm}
\begin{rem} \ \begin{enumerate}
\item Hölder's inequality is the reason why probability theory behaves "nicer" in consideration of Lebesgue spaces, i.e. let $0<r<s$ and set $p= \frac{s}{r}$ then for the conjugate $q=\frac{p}{p-1}$ we can apply Hölder's inequality to $|X|^r$ and the constant $1$ function. 
\begin{align*}
\mathbb{E}(|X|^r) \leq \mathbb{E}(|X|^{rp})^{1/p} = \mathbb{E}(|X|^s)^{r/s} \implies \mathbb{E}(|X|^r)^{\frac{1}{r}} \leq \mathbb{E}(|X|^s)^{\frac{1}{s}} 
\end{align*}
in particular if $X$ is a random variable such that $X \in L^s$ then it always follows that $X \in L^r$ for any $r <s$. In general measure theory, where the spaces don't need to be of finite measure, this is not the case at all. 
\item It is an important Theorem of Riesz that states that for all $1 \leq p \leq \infty$ the $L^p$ spaces are complete, in particular they are Banach spaces. Especially for $p=2=q$ the space $L^2$ is even a Hilbert space. 
\end{enumerate}
\end{rem}
\begin{prop}[Jensen's inequality] Let $X \in L^1$ and $\varphi: \mathbb{R} \to \mathbb{R}$ a convex function, then we have 
\begin{align*}
\varphi( \mathbb{E}(X)) \leq \mathbb{E}( \varphi(X))
\end{align*}
\end{prop}
\begin{prop}[Existence of the conditional expection] Let $\mathcal{G} \subset \mathcal{F}$ be a subsigma-algebra. We define $Z= \mathbb{E}(Z \mid \mathcal{G})$ to be the unique $\mathcal{G}$-measurable variable such that 
\begin{align*}
\mathbb{E}(Z 1_A) = \mathbb{E}(X 1_A) \text{ for all } A \in \mathcal{G}
\end{align*}
\end{prop}
\newpage
\section{Discrete time Martingales}
\begin{defn} Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, an increasing sequence of $\sigma$-algebras $\mathcal{F}_0 \subset \mathcal{F}_1 \subset \mathcal{F}_2 \subset \cdots \subset \mathcal{F}$ is called a filtration on $\Omega$. We call the space $(\Omega, \mathcal{F}, (\mathcal{F})_{n \in \mathbb{N}}, \mathbb{P})$ a filtered probability space. 
\end{defn}
\begin{defn} A stochastic process $X=(X_n)_{n \in \mathbb{N}}$ is called adapted if $X_n$ is $\mathcal{F}_n$-measurable for all $n \in \mathbb{N}$. Moreoever, an adapted process is called a martingale if $X_n \in L^1$ for all $n \in \mathbb{N}$ and 
\begin{align*}
\mathbb{E}(X_n \mid \mathcal{F}_m) = X_{n \wedge m } \text{ for all } n,m \geq 0 
\end{align*}
\end{defn}
\begin{rem} Analogeously we define sub-martingales if instead of an equality we have $\geq$ above, or a super-martingale for $\leq$ respectively. 
\end{rem}
\begin{defn} A stopping time $T$ is a random variable $T: \Omega \to \mathbb{N}_0^\infty$ such that $\lbrace T \leq n \rbrace \in \mathcal{F}_n$ for all $n \in \mathbb{N}$. The sigma-algebra at the stopping time $T$ is then defined as
\begin{align*}
\mathcal{F}_T := \lbrace A \in \mathcal{F}: A \cap \lbrace T \leq n \rbrace \in \mathcal{F}_n \text{ for all } n \in \mathbb{N} \rbrace 
\end{align*}
\end{defn}
\begin{rem} The sigma algebra at the stopping time $T$ $\mathcal{F}_T$ encodes the information up to the random time $T$. In other words, if we interprete our filtered probability space as an random experiment, then the maximum information that can be found until the random time $T$ is in $\mathcal{F}_T.$
\end{rem}
\begin{thm}[Optional stopping theorem - weak version] Let $X$ be a martingale. Let $X^T:=(X_n^T)_{n \in \mathbb{N}}$ be given by $X_n^T:= X_{n \wedge T}$ then $X^T$ is also a martingale. Moreover if $S, T$ are almost surely finite stopping times with $S \leq T$ almost surely, then
\begin{align*}
\mathbb{E}(X_T \mid \mathcal{F}_S) = X_S
\end{align*}
\end{thm}
\begin{thm}[Martingale convergence Theorem] Let $X$ be a super-martingale that is bounded in $L^1$ (i.e. $\sup_{n \in \mathbb{N}} \mathbb{E}(|X_n|) < \infty$). Then there exists a random variable $X_\infty \in L^1$ such that $X_n \to X_\infty$ almost surely, moreover 
\begin{align*}
\mathbb{E}(|X_\infty|) \leq \sup_{n \in \mathbb{N}} \mathbb{E}(|X_n|)
\end{align*}
\end{thm}
\begin{cor} Let $X$ be a super-martingale that is bounded from below (i.e. $X_n \geq c$ a.s. for some $c \in \mathbb{R}$ for all $n \in \mathbb{N}$). Then $X_n$ converges almost surely to $X_\infty \in L^1$. 
\end{cor}
\begin{thm}[Behaviour of Martingales with bounded increments] Let $X=(X_n)_{n \in \mathbb{N}}$ be a martingale with bounded increments (i.e. $|X_{n+1}-X_n| \leq C$). Let $\mathcal{C}:= \lbrace \limsup X_n = \liminf X_n \in \mathbb{R} \rbrace$ , $\mathcal{O}:= \lbrace \limsup X_n = + \infty \rbrace \cap \lbrace \liminf X_n = - \infty \rbrace $, then $\mathbb{P}( \mathcal{O} \cup \mathcal{C}) = 1$.  
\end{thm}
\begin{lem}[Doob's inequalities] Let $X$ be a non-negative submartingale and $X_n^*:= \max_{k \leq n} X_k$ for $n \in \mathbb{N}$, then we have for any $\lambda >0$
\begin{align*}
\lambda \mathbb{P}(X_n^* > \lambda)  \leq \mathbb{E}(X_n 1_{X_n^* > \lambda})  \leq \mathbb{E}(X_n) 
\end{align*}
In addition, for $p > 1$, we have
\begin{align*}
\|X_n^*\|_p \leq \frac{p}{p-1} \| X_n \|_p 
\end{align*}
\end{lem}
\begin{thm}[Closed martingale convergence theorem] Let $X$ be a martingale and $p>1$. Then the following statements are equivalent:
\begin{enumerate}
\item $\sup_{n \in \mathbb{N}} \|X_n\|_p < \infty$ 
\item $X_n$ converges almost surely and in $L^p$ to $X_\infty \in L^p$ 
\item There exists a random varaible $X_\infty \in L^p$ such that 
\begin{align*}
\mathbb{E}(X_\infty \mid \mathcal{F}_n) = X_n \text{ for all } n \in \mathbb{N}
\end{align*}
\end{enumerate}
If any (and consequently all) of these conditions hold true, we say that $X$ is a closed martingale in $L^p.$
\end{thm}
\begin{defn} A family $\mathcal{H}$ of random variables is called uniformly integrable (UI) if 
\begin{align*}
\lim_{\lambda \to \infty} \sup_{X \in \mathcal{H}} \mathbb{E}( |X| 1_{|X| > \lambda} ) = 0 
\end{align*}
\end{defn}
\noindent \textbf{Exercise:} Prove that $\mathcal{H}$ is UI if there exists $G: [0, \infty) \to [0, \infty)$ non-decreasing such that 
\begin{enumerate}
\item $\lim_{t \to \infty} \frac{G(t)}{t} = \infty$ 
\item $\sup_{X \in \mathcal{H}} \mathbb{E}(G(X)) \infty$ 
\end{enumerate}
\begin{thm}[UI convergence Theorem] Let $(X_n)_{n \in \mathbb{N}}$ a stochastic process, then the following statements are equivalent:
\begin{enumerate}
\item $(X_n)_{n \in \mathbb{N}}$ is UI and $X_n \to X_\infty$ in probability.
\item $X_n \to X_\infty$ in $L^1$.
\end{enumerate}
\end{thm}
\begin{thm}[Characterisation of UI martingales through closedness] Let $X$ be a martingale. The following assertions are equivalent:
\begin{enumerate}
\item $X$ is UI. 
\item $X$ converges almost surely and in $L^1$ to $X_\infty \in L^1.$ 
\item There exists $X_\infty \in L^1$ such taht $\mathbb{E}(X_\infty \mid \mathcal{F}_n) = X_n$, i.e. the martingale is closed. 
\end{enumerate}
\end{thm}
\begin{thm}[Optional stopping - strong version] Let $X$ be a closed (and thus UI) martingale. Then for any stopping times $S$ and $T$, one has 
\begin{align*}
\mathbb{E}(X_T \mid \mathcal{F}_S)= X_{T \wedge S}
\end{align*}
\end{thm}
\newpage
\begin{defn} Let $\mathcal{F}$ be a sigma-algebra and consider a sequence \\ $\dots \subset \mathcal{H}_2 \subset \mathcal{H}_1 \subset \mathcal{F}$ of sigma-algebras (i.e. we lose information over time). Let $Y_1 \in L^p( \Omega, \mathcal{H}_1, \mathbb{P})$ for some $p \in [1, \infty)$. Then $Y=(Y_n)_{n \in \mathbb{N}}$ is called a backward-martingale if $Y_n= \mathbb{E}(Y_1 \mid \mathcal{H}_n)$ for all $n \in \mathbb{N}$. 
\end{defn}
\begin{thm}[Convergence theorem for backward-martingales] \ \newline Let $Y=(Y_n)_{n \in \mathbb{N}}$ be a backward-martingale. Then $Y$ converges almost surely and in $L^p$ to $Y_\infty = \mathbb{E}(Y_1 \mid \mathcal{H}_\infty )$ where $\mathcal{H}_\infty = \cap_{n \in \mathbb{N}} \mathcal{H}_n$. 
\end{thm}
\begin{thm}[Kolmogorov's Law of Large Numbers] Let $(\xi_n)_{n \in \mathbb{N}}$ be a sequence of i.i.d. random variables with $\xi_1 \in L^1$. Let $S_n = \xi_1 + \dots + \xi_n$ be the simple random walk, then we have
\begin{align*}
\frac{S_n}{n} \to \mathbb{E}(X_1) \text{ almost surely and in } L^1
\end{align*}
\end{thm}
\begin{proof}
\textbf{Exercise.} Kolmogorov's LLN can be shown in it's full generality (i.e. for $L^1$ RV) by using backwards martingales and Kolomogorov's 0-1 Law. 
\end{proof}
\begin{thm}[Kolmogorov's 0-1 Law] Let $(\xi_n)_{n \in \mathbb{N}}$ be a sequence of i.i.d. random variables and let $\mathcal{H}_n = \sigma( \xi_j : j \geq n )$. Then, the sigma algebra $\mathcal{H}_\infty := \cup_{n \in \mathbb{N}} \mathcal{H}_n$ is trivial, i.e. for every event $A \in \mathcal{H}_\infty$ we have  \\ $\mathbb{P}(A) \in \lbrace 0 ,1 \rbrace$. 
\end{thm}
\noindent Next we will remind the very important Borel-Cantelli Lemmas. We recall the following definiton. Let $(A_n)_{n \in \mathbb{N}}$ be a sequence of events in a probability space $( \Omega, \mathcal{F}, \mathbb{P})$ then we define 
\begin{align*}
A & = \limsup_{n \to \infty} A_n:= \bigcap_{n=1}^\infty \bigcup_{i \geq n}^\infty A_i = \lbrace A_n \text{ infinitely often} \rbrace  
\\
A^c &:= \liminf_{n \to \infty} A_n := \bigcup_{n=1}^\infty \bigcap_{i \geq n }^\infty A_i = \lbrace A_n \text{ eventually} \rbrace 
\end{align*}
We remark that the following is true
\begin{align*}
\omega \in \bigcap_{n=1}^\infty \bigcup_{i \geq n}^\infty A_i &\iff \forall n \in \mathbb{N}, \exists i \geq n : \omega \in A_i   \\
\omega \in \bigcup_{n=1}^\infty \bigcap_{i \geq n }^\infty A_i^c &\iff \exists n \in \mathbb{N}, \forall i \geq n : \omega \in A_i
\end{align*}
\begin{thm}[Borel-Cantelli Lemma 1] If $\sum_{n \in \mathbb{N}} \mathbb{P}(A_n) < \infty$, then we have $\mathbb{P}(A_n \text{ i.o.} \rbrace=0$.
\end{thm}
\begin{thm}[Borel-Cantelli Lemma 2] If the events $A_n$ are independent and $\sum_{n \in \mathbb{N}} \mathbb{P}(A_n) = \infty$, then we have $\mathbb{P}( A_n \text{ i.o.} \rbrace = 1$ 
\end{thm}
\newpage
\begin{thm}[Extension of Borel Cantelli Lemma] Let $\mathcal{F}= (\mathcal{F}_n)_{n = 1}^\infty$ be a Filtration and suppose $A_n \in \mathcal{F}_n$, then we have
\begin{align*}
 \lbrace A_n \text{ i.o.}\rbrace = \left \lbrace \sum_{n \geq k} \mathbb{P}(A_n \mid \mathcal{F}_{n-1} ) = \infty \right \rbrace 
\end{align*}
for all $k \in \mathbb{N}$ up to a set of measure zero. 
\end{thm}
\noindent We now give some results that are related to the Central Limit Theorem (CLT). We will first recall the classical result.
\begin{thm}[CLT] Let $X_1, X_2, \dots$ be a sequence of i.i.d. $L^2$ random variables with $\mathbb{E}(X)= \mu$ and Var$(X)= \sigma >0$. Let $S_n= X_1 + \dots + X_n$, then we have
\begin{align*}
\frac{S_n-n \mu }{n \sigma} \implies \mathcal{N}(0,1) 
\end{align*}
\end{thm}
\noindent It is our goal to generalize this result, i.e. weaken the conditions and then give a central limit theorem for martingales. 
\\\\
The outline is the following: Let $\lbrace X_{n,k} : 1 \leq k \leq J(n), n \in \mathbb{N}_{ \geq 1 } \rbrace$ be an array of centered random variables. Here $K$ can be random, however it is almost surely finite with $J(n) \to \infty$ as $n \to \infty$.  
\\\\
Let $( \xi_j)_{j \in \mathbb{N}}$ be a sequence of (centered?) i.i.d. random variables, we then set 
\begin{align*}
X_{n,k} = \frac{\xi_k}{\sqrt{n}}
\end{align*}
Suppose that $X_{n,k} \in L^2( \mathcal{F})$ and let \begin{align*}
S_n := \sum_{k=1}^{J(n)} X_{n,k}
\end{align*}
\begin{thm}[Mc Leish] In the situation as above, let us define \\ $T_n = \prod_{k=1}^{J(n)} (1 + it X_{n,k})$ and suppose that
\begin{enumerate}
\item $T_n$ is UI and $\mathbb{E}(T_n) \to 1$ as $n \to \infty$.
\item $ \sum_{k=1}^{J(n)} X_{n,k}^2 \to 1$ in Probability.
\item $\max_{k \leq n} |X_{n,k}| \to 0$ in Probability. 
\end{enumerate}
Then $S_n \implies \mathcal{N}(0,1)$ as $n \to \infty$. 
\end{thm}
\newpage
\begin{thm}[Martingale Central Limit Theorem] Let $X_{n,k}$ where $1 \leq k \leq m_n$ ($m_n$ is deterministic with $m_n \to \infty$ as $n \to \infty$) be a $\mathcal{F}_{n,k}$-measurable martingale difference array. Suppose that 
\begin{enumerate}
\item $\mathbb{E}(X_{n,k} \mid \mathcal{F}_{n,k-1}) = 0$ 
\item $V_n:= \sum_{j=1}^{m_n} X_{n,j}^2 \to 1$ in Probability.
\item (Lindenberg Condition) For any $\epsilon >0$ we have that 
\begin{align*}
\lim_{n \to \infty}  \sum_{k=1}^n \mathbb{E}(X_{n,k}^2 1_{ | X_{n,k}| > \epsilon}) = 0 
\end{align*}
\end{enumerate}
Then we have as $n \to \infty$ 
\begin{align*}
S_n=  \sum_{k=1}^{m_n} X_{n,k} \implies \mathcal{N}(0,1) 
\end{align*}
\end{thm}
\end{document}
